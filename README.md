# VoiceAgent

A real-time voice agent built with OpenAI (LLM), Deepgram (STT + TTS), and Python.

---

## Architecture Overview

![Architecture Overview](assets/architecture_overview.png)

Incoming user messages pass through the **Session Analyzer**, which decides which memory strategy to activate. All three strategies feed into a **unified prompt builder** before the LLM call is made.

---

## The Context Stack

![Context Stack](assets/context_stack.png)

Every LLM call is built from four ordered layers:

1. **System Prompt** ‚Äî persona, rules, and knowledge base. Never trimmed.
2. **Rolling Summary** ‚Äî compressed record of what was discussed in earlier turns.
3. **Recent Turns** ‚Äî the exact conversation window (last N turns via sliding window).
4. **Current User Input** ‚Äî the live message being responded to.

---

## Project Structure

```
voice_agent/
‚îÇ
‚îú‚îÄ‚îÄ main.py                        # Entry point ‚Äî boots the app
‚îÇ
‚îú‚îÄ‚îÄ config.py                      # All constants and env vars in one place
‚îÇ
‚îú‚îÄ‚îÄ .env                           # API keys (never commit this)
‚îÇ
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ architecture_overview.png  # Architecture diagram
‚îÇ   ‚îî‚îÄ‚îÄ context_stack.png          # Context stack diagram
‚îÇ
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ voice_agent.py             # VoiceAgent class, main run loop
‚îÇ   ‚îú‚îÄ‚îÄ turn.py                    # _turn(), sentence splitting logic
‚îÇ   ‚îî‚îÄ‚îÄ interruption.py            # Interruption detection and recovery
‚îÇ
‚îú‚îÄ‚îÄ session/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ manager.py                 # SessionManager class
‚îÇ   ‚îú‚îÄ‚îÄ state.py                   # VoiceSession dataclass
‚îÇ   ‚îú‚îÄ‚îÄ window.py                  # Sliding window logic
‚îÇ   ‚îú‚îÄ‚îÄ summarizer.py              # Summarization strategy
‚îÇ   ‚îî‚îÄ‚îÄ entities.py                # Entity extraction strategy
‚îÇ
‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ capture.py                 # Mic capture
‚îÇ   ‚îú‚îÄ‚îÄ playback.py                # Speaker playback
‚îÇ   ‚îî‚îÄ‚îÄ config.py                  # AUDIO_CHANNELS, SAMPLE_RATES, etc.
‚îÇ
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ stream.py                  # OpenAI streaming
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                 # All prompt templates in one place
‚îÇ
‚îú‚îÄ‚îÄ stt/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ deepgram.py                # Deepgram socket setup, _on_transcript
‚îÇ
‚îú‚îÄ‚îÄ tts/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ deepgram.py                # _speak(), TTS HTTP call
‚îÇ
‚îî‚îÄ‚îÄ knowledge/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ loader.py                  # load_knowledge()
    ‚îî‚îÄ‚îÄ knowledge.json             # Your knowledge base
```

---

## File Status

| File | Status | Notes |
|------|--------|-------|
| `config.py` | ‚úÖ Ready | All constants and env vars |
| `main.py` | ‚úÖ Ready | Entry point only |
| `agent/voice_agent.py` | ‚úÖ Ready | Core agent loop |
| `audio/capture.py` | ‚úÖ Ready | Mic capture via miniaudio |
| `audio/playback.py` | ‚úÖ Ready | Speaker playback via miniaudio |
| `llm/stream.py` | ‚úÖ Ready | OpenAI streaming with TTFT logging |
| `llm/prompts.py` | ‚úÖ Ready | System prompt + greeting |
| `stt/deepgram.py` | ‚úÖ Ready | Deepgram Nova-3 live socket |
| `tts/deepgram.py` | ‚úÖ Ready | Deepgram Aura-2 TTS |
| `knowledge/loader.py` | ‚úÖ Ready | JSON knowledge base loader |
| `session/state.py` | ‚úÖ Ready | VoiceSession dataclass |
| `session/manager.py` | üîÑ Partial | `create()` and `get()` done |
| `agent/turn.py` | ‚è≥ Pending | Needs session layer first |
| `agent/interruption.py` | ‚è≥ Pending | Needs session layer first |
| `session/window.py` | ‚è≥ Pending | Sliding window logic |
| `session/summarizer.py` | ‚è≥ Pending | Long session summarization |
| `session/entities.py` | ‚è≥ Pending | Entity extraction + memory |

---

## Audio Settings

| Parameter | Value | Reason |
|-----------|-------|--------|
| `AUDIO_CHANNELS` | 1 (Mono) | Standard for voice |
| `MICROPHONE_SAMPLE_RATE` | 16,000 Hz | Matches Deepgram STT input |
| `SPEAKER_SAMPLE_RATE` | 24,000 Hz | Matches Deepgram TTS output |
| `BUFFER_CHUNK_SIZE_MS` | 30ms | Low latency, stable throughput |
| `BUFFER_CHUNK_SIZE_BYTES` | 960 bytes | Derived: `16000 * 1 * 2 * 0.03` |

---

## Architecture Overview

![Architecture Overview](assets/architecture_overview.png)

Incoming user messages pass through the **Session Analyzer**, which decides which memory strategy to activate. All three strategies feed into a **unified prompt builder** before the LLM call is made.

---

## Session Management Strategy

The agent uses a combined three-layer strategy for managing conversation history:

**1. Sliding Window** ‚Äî always active. Keeps only the last N turns in the active prompt to control token usage.

**2. Summarization** ‚Äî triggered when history exceeds a threshold. Compresses old turns into a rolling summary instead of dropping them.

**3. Entity Memory** ‚Äî runs every N turns. Extracts key facts (name, preferences, intent) and persists them across the full session regardless of window size.

### Trigger Thresholds

| Trigger | Condition | Activates |
|---------|-----------|-----------|
| Always | Every turn | Sliding window |
| Always | Entities exist | Entity injection into prompt |
| Every 2 turns | `turn_count % 2 == 0` | Entity extraction |
| History overflow | Turns since last summary >= 10 | Summarization |
| Token overflow | Total tokens >= 2000 | Summarization |
| Idle timeout | Silence >= 5 minutes | Summarization |

---

## The Context Stack

![Context Stack](assets/context_stack.png)

Every LLM call is built from four ordered layers:

1. **System Prompt** ‚Äî persona, rules, and knowledge base. Never trimmed.
2. **Rolling Summary** ‚Äî compressed record of what was discussed in earlier turns.
3. **Recent Turns** ‚Äî the exact conversation window (last N turns via sliding window).
4. **Current User Input** ‚Äî the live message being responded to.

---

## Setup

### 1. Install Dependencies

```bash
pip install openai deepgram-sdk miniaudio requests python-dotenv
```

### 2. Configure Environment

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_key_here
DEEPGRAM_API_KEY=your_deepgram_key_here
```

### 3. Add Knowledge Base

Create `knowledge/knowledge.json` with your domain knowledge:

```json
{
  "company": "Acme Corp",
  "product": "Voice Assistant v1",
  "faq": {
    "hours": "We are open 9am to 5pm EST.",
    "refund": "Refunds are processed within 5-7 business days."
  }
}
```

### 4. Run

```bash
python main.py
```

---

## How It Works

```
User speaks
    ‚Üì
Mic captures audio (16kHz PCM, 30ms chunks)
    ‚Üì
Deepgram STT (Nova-3) ‚Üí transcript
    ‚Üì
Interruption check ‚Üí if agent speaking, stop playback
    ‚Üì
Session history + system prompt ‚Üí OpenAI (gpt-4o-mini) streaming
    ‚Üì
Sentence-by-sentence ‚Üí Deepgram TTS (Aura-2)
    ‚Üì
Audio plays on speaker (24kHz PCM)
```

---

## Implementation Phases

| Phase | What | Status |
|-------|------|--------|
| Phase 1 | Complete SessionManager (`end`, `cleanup_expired`) | ‚è≥ Next |
| Phase 2 | History management (`window.py`, `add_turn`) | ‚è≥ |
| Phase 3 | Connect session to VoiceAgent | ‚è≥ |
| Phase 4 | Interruption recovery (`interruption.py`) | ‚è≥ |
| Phase 5 | Summarization (`summarizer.py`) | ‚è≥ |
| Phase 6 | Entity memory (`entities.py`) | ‚è≥ |
| Phase 7 | Persistence (Redis / JSON store) | ‚è≥ |

---

## Models Used

| Component | Model |
|-----------|-------|
| LLM | `gpt-4o-mini` |
| STT | Deepgram `nova-3` |
| TTS | Deepgram `aura-2-thalia-en` |